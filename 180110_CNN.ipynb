{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed-Forward Neural Network(FFNN)** : input으로 $x$를 받아 $y=Wx + b$ 계산하고 이 값에 **activation function** 적용\n",
    "\n",
    "**Multilayer Perceptron(MLP)** : Layer를 여러 개 쌓아올린 구조\n",
    "\n",
    "    학습시간, 변수의 개수, 망의 크기(Network size)가 과도하게 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**의 경우 모든 노드들이 아닌 일부분, 즉 **receptive field**에 각각 **weight**를 적용함\n",
    "\n",
    "    이미지를 긴 벡터로 바꾸는 MLP와는 달리, CNN은 28*28 행렬을 그대로 이용함\n",
    "    3*3 커널 윈도우, 즉 행렬로 sweep함으로써 데이터를 다른 형태로 변환함(이 과정이 convolution)\n",
    "    \n",
    "    convolution은 주위값들을 반영해서 중앙값을 변화시켜나가는 것 : 목적하는 작업의 성공률이 높도록 이미지를 변형\n",
    "    이렇게 변형된 이미지를 feature map이라고 함 : 이러한 feature map에 대해 classification을 진행함\n",
    "    결국, CNN은 원본에 대한 다양한 커널행렬을 통해 다양한 feature map을 생성하고 여기에 대해 학습을 진행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**용어들**\n",
    "\n",
    "+ width   : 이미지의 가로, 세로 픽셀사이즈\n",
    "+ channel : 흑백일 경우 1, RGB일 경우 3\n",
    "+ 기본적 데이터 형태 : [이미지 수, width, width, channel]\n",
    "+ hidden : fully connected layer의 hidden neuron 개수\n",
    "+ pooling : '골라내기', 즉 사이즈를 줄이는 과정\n",
    "+ conv-ReLu-pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunwoogu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = 10\n",
    "width = 28\n",
    "channels = 1\n",
    "valid = 10000\n",
    "\n",
    "steps = 1000\n",
    "batch = 100  # stochastic gradient descent batch size\n",
    "patch = 5    # convolutional kernel size\n",
    "depth = 8    # convolutional kernel depth size = # of convolutional kernels\n",
    "hidden= 100  # of hidden neurons in the fully connected layer\n",
    "\n",
    "alpha = .001 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data   = pd.read_csv('train.csv')    # 42000 * 785\n",
    "\n",
    "Labels = np.array(Data.pop('label')) # popping out the label column\n",
    "Labels = LabelEncoder().fit_transform(Labels)[:, None]\n",
    "Labels = OneHotEncoder().fit_transform(Labels).todense()\n",
    "\n",
    "Data = StandardScaler().fit_transform(np.float32(Data.values)) # dataframe into numpy array\n",
    "Data = Data.reshape(-1, width, width, channels) # data into 2d images\n",
    "\n",
    "TrainData,   ValidData = Data[:-valid], Data[-valid:]\n",
    "TrainLabels, ValidLabels = Labels[:-valid], Labels[-valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape = (32000, 28, 28, 1) = (TRAIN, WIDTH, WIDTH, CHANNELS)\n",
      "labels shape = (42000, 10) = (TRAIN, LABELS)\n"
     ]
    }
   ],
   "source": [
    "print('train data shape = ' + str(TrainData.shape) + ' = (TRAIN, WIDTH, WIDTH, CHANNELS)')\n",
    "print('labels shape = ' + str(Labels.shape) + ' = (TRAIN, LABELS)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfData = tf.placeholder(tf.float32, shape=(None, width, width, channels))\n",
    "tfLabels = tf.placeholder(tf.float32, shape=(None, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.truncated_normal([patch, patch, channels, depth], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([depth]))\n",
    "w2 = tf.Variable(tf.truncated_normal([patch, patch, depth, 2*depth], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(1.0, shape=[2*depth]))\n",
    "w3 = tf.Variable(tf.truncated_normal([width // 4 * width // 4 * 2*depth, hidden], stddev=0.1))\n",
    "b3 = tf.Variable(tf.constant(1.0, shape=[hidden]))\n",
    "w4 = tf.Variable(tf.truncated_normal([hidden, labels], stddev=0.1))\n",
    "b4 = tf.Variable(tf.constant(1.0, shape=[labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logits(data):\n",
    "    # Convolutional layer 1\n",
    "    x = tf.nn.conv2d(data, w1, [1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    x = tf.nn.relu(x + b1)\n",
    "    # Convolutional layer 2\n",
    "    x = tf.nn.conv2d(x, w2, [1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    x = tf.nn.relu(x + b2)\n",
    "    # Fully connected layer\n",
    "    x = tf.reshape(x, (-1, width // 4 * width // 4 * 2*depth))\n",
    "    x = tf.nn.relu(tf.matmul(x, w3) + b3)\n",
    "    return tf.matmul(x, w4) + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfPred = tf.nn.softmax(logits(tfData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits(tfData), labels=tfLabels))\n",
    "tfAcc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(tfPred, 1), tf.argmax(tfLabels, 1))))\n",
    "tfOpt = tf.train.RMSPropOptimizer(alpha)\n",
    "tfStep = tfOpt.minimize(tfLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunwoogu/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 \t Valid. Acc. = 96.639999\n",
      "Step 1000 \t Valid. Acc. = 97.320000\n"
     ]
    }
   ],
   "source": [
    "ss = ShuffleSplit(n_splits=steps, train_size=batch)\n",
    "ss.get_n_splits(TrainData, TrainLabels)\n",
    "\n",
    "history = [(0, np.nan, 10)]\n",
    "\n",
    "for step, (idx, _) in enumerate(ss.split(TrainData,TrainLabels), start=1):\n",
    "    fd = {tfData:TrainData[idx], tfLabels:TrainLabels[idx]}\n",
    "    session.run(tfStep, feed_dict=fd)\n",
    "    if step%500 == 0:\n",
    "        fd = {tfData:ValidData, tfLabels:ValidLabels}\n",
    "        validLoss, validAccuracy = session.run([tfLoss, tfAcc], feed_dict=fd)\n",
    "        history.append((step, validLoss, validAccuracy))\n",
    "        print('Step %i \\t Valid. Acc. = %f'%(step, validAccuracy), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test = pd.read_csv('test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TestData = StandardScaler().fit_transform(np.float32(Test.values)) # Convert the dataframe to a numpy array\n",
    "TestData = TestData.reshape(-1, width, width, channels) # Reshape the data into 42000 2d images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TestPred = session.run(tfPred, feed_dict={tfData:TestData})\n",
    "TestLabels = np.argmax(TestPred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Prediction: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwr9EyH+hkAWcggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a317da588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 10\n",
    "print(\"Label Prediction: %i\"%TestLabels[10])\n",
    "\n",
    "fig = plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(TestData[k,:,:,0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
